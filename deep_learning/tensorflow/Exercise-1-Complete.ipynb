{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing your first Deep Learn Convolutional network\n",
    "This tutorial will give you the classification method in order to classify cats and dogs! Good luck!\n",
    "\n",
    "First let's start by downloading the data that is available publicly from Kaggle:\n",
    "\n",
    "https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data\n",
    "\n",
    "## Prepare the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # dealing with directories\n",
    "from random import (\n",
    "    shuffle,\n",
    ")  # mixing up or currently ordered data that might lead our network astray in training.\n",
    "\n",
    "import cv2  # working with, mainly resizing, images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # dealing with arrays\n",
    "import tensorflow as tf\n",
    "from tqdm import (\n",
    "    tqdm,\n",
    ")  # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion\n",
    "\n",
    "TRAIN_DIR = \"train\"\n",
    "TEST_DIR = \"test\"\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3\n",
    "\n",
    "MODEL_NAME = \"dogsvscats-{}-{}.model\".format(\n",
    "    LR, \"2conv-basic\"\n",
    ")  # just so we remember which saved model is which, sizes must match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing label data\n",
    "\n",
    "We are going yo make one-hot vectors for cat and dog. The cat will have the vector representation of [1, 0] and the dog will have the [1, 0] representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split(\".\")[-3]\n",
    "    # conversion to one-hot array [cat,dog]\n",
    "    #                            [much cat, no dog]\n",
    "    if word_label == \"cat\":\n",
    "        return [1, 0]\n",
    "    #                             [no cat, very doggo]\n",
    "    elif word_label == \"dog\":\n",
    "        return [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing train data\n",
    "\n",
    "In this segment the data must be read from the filesystem, and resized to have the same size. In this example we are also going to make a gratscale of the image in order to reduce the number of channels and, therefore, processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        training_data.append([np.array(img), np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save(\"train_data.npy\", training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data\n",
    "\n",
    "This is the same processing but for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(TEST_DIR)):\n",
    "        path = os.path.join(TEST_DIR, img)\n",
    "        img_num = img.split(\".\")[0]\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "\n",
    "    shuffle(testing_data)\n",
    "    np.save(\"test_data.npy\", testing_data)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Comment the first and uncomment the second line if this has been already done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_train_data()\n",
    "# If you have already created the dataset:\n",
    "# train_data = np.load('train_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the architecture!\n",
    "\n",
    "Now the exercise will need you to:\n",
    "1. make two placeholders called input and output with sizes of (None, width, height, channels) and (None) dimensions  respectfully\n",
    "2. make 3 convnets using tf.layers.conv2d and insert the input, filters, kernel_size, strides and name\n",
    "3. after each convnets put a max pooling layer using tf.layers.max_pooling2d and check dimensions\n",
    "4. after that use one dense layer of 1024 units and a relu activation\n",
    "5. make a last variable called \"logits\" that has the last dense layer as input and no activation function and only 2 units (number of classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 16  # number of filters\n",
    "kernel_size = [3, 3]  # the size of each filter\n",
    "strides = [1, 1]  # how many \"skips\" are going to be made\n",
    "\n",
    "input = tf.placeholder(\n",
    "    dtype=tf.float32, shape=[None, IMG_SIZE, IMG_SIZE, 1]\n",
    ")  # placeholder for the input (don't forget you need this one)\n",
    "output = tf.placeholder(dtype=tf.int32, shape=[None, 2])  # placeholder for the output\n",
    "\n",
    "convnet = tf.layers.conv2d(input, filters, kernel_size, strides, name=\"conv1\")\n",
    "convnet = tf.layers.max_pooling2d(convnet, kernel_size, [3, 3])\n",
    "\n",
    "convnet = tf.layers.conv2d(convnet, filters * 2, kernel_size, strides, name=\"conv2\")\n",
    "convnet = tf.layers.max_pooling2d(convnet, kernel_size, [2, 2])\n",
    "\n",
    "convnet = tf.layers.conv2d(convnet, filters * 4, kernel_size, strides, name=\"conv3\")\n",
    "convnet = tf.layers.max_pooling2d(convnet, kernel_size, [4, 4])\n",
    "\n",
    "dense = tf.layers.dense(\n",
    "    tf.reshape(convnet, (-1, filters * 4)), 1024, activation=tf.nn.relu\n",
    ")  # for cats == 0 and dogs == 1\n",
    "logits = tf.layers.dense(dense, 2)  # for cats == 0 and dogs == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for training!!\n",
    "\n",
    "Now we are going to prepare the optimizer function:\n",
    "1. create a variable pred that will be the result of the classification\n",
    "2. make the loss function using softmax_cross_entropy\n",
    "3. make the variable op that is the minimization of the AdamOptimizer in realtionm to the loss\n",
    "4. Open a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.argmax(logits, axis=1)\n",
    "loss = tf.losses.softmax_cross_entropy(output, logits)\n",
    "op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just a few preparations left to input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some parameters\n",
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "Loss = []\n",
    "\n",
    "train_percent = 0.6\n",
    "train_num = int(train_percent * len(train_data))\n",
    "train = train_data[:train_num]\n",
    "test = train_data[train_num:]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_y = np.array([i[1] for i in test])\n",
    "\n",
    "print(X.shape)\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!!\n",
    "\n",
    "1. Now using session.run calculate the loss and the operation of optimization;\n",
    "2. Use the feed_dict as {\"placeholder1 name\": \"numpy input\", \"placeholder1 name\": \"numpy input\"}\n",
    "3. Calculate the accuracy\n",
    "3. Monitorize the loss and the accuracy\n",
    "4. After checking everything is ok, implement mini-batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# fig2, ax2 = plt.subplots()\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(n_epochs):\n",
    "    indexes = np.random.permutation(len(X))\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        _, l = session.run(\n",
    "            [op, loss],\n",
    "            feed_dict={\n",
    "                input: X[indexes[i : i + batch_size]],\n",
    "                output: Y[indexes[i : i + batch_size]],\n",
    "            },\n",
    "        )\n",
    "        Loss.append(l)\n",
    "        # ax.clear()\n",
    "        # ax.plot(Loss)\n",
    "        # plt.pause(0.1)\n",
    "    print(\"Loss:\", l)\n",
    "\n",
    "    full_loss, prediction = session.run([loss, pred], feed_dict={input: X, output: Y})\n",
    "\n",
    "    acc = np.sum(prediction == np.argmax(Y, axis=1)) * 1.0 / len(Y)\n",
    "    print(\"acc:\", acc)\n",
    "    # ax2.clear()\n",
    "    # ax2.plot(acc)\n",
    "    # plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implementing keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class AccuracyHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get(\"acc\"))\n",
    "\n",
    "\n",
    "history = AccuracyHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# from keras import\n",
    "# from keras.layers import *\n",
    "num_classes = 2\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(\n",
    "    layers.Conv2D(\n",
    "        32,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(1, 1),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 1),\n",
    "    )\n",
    ")\n",
    "model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation=\"relu\"))\n",
    "model.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=LR),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "print(np.array(Y))\n",
    "model.fit(\n",
    "    np.array(X),\n",
    "    np.array(Y),\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=[history],\n",
    ")\n",
    "\n",
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
