{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# check our work directory\n",
    "import os\n",
    "\n",
    "# to unzip datasets\n",
    "import zipfile\n",
    "\n",
    "# visualize some datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# using numpy\n",
    "import numpy as np\n",
    "\n",
    "# for data load or save\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "plt.style.use([\"dark_background\", \"bmh\"])\n",
    "plt.rc(\"axes\", facecolor=\"k\")\n",
    "plt.rc(\"figure\", facecolor=\"k\")\n",
    "plt.rc(\"figure\", figsize=(10, 10), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(1234)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../data/dogs-vs-cats-redux-kernels-edition\"\n",
    "with zipfile.ZipFile(os.path.join(base_dir, \"train.zip\")) as train_zip:\n",
    "    train_zip.extractall(\"../data/dogs-vs-cats-redux-kernels-edition\")\n",
    "with zipfile.ZipFile(os.path.join(base_dir, \"test.zip\")) as test_zip:\n",
    "    test_zip.extractall(\"../data/dogs-vs-cats-redux-kernels-edition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../../data/dogs-vs-cats-redux-kernels-edition/train\"\n",
    "test_dir = \"../../data/dogs-vs-cats-redux-kernels-edition/test\"\n",
    "\n",
    "train_list = glob.glob(os.path.join(train_dir, \"*.jpg\"))\n",
    "test_list = glob.glob(os.path.join(test_dir, \"*.jpg\"))\n",
    "\n",
    "print(f\"Training samples: {len(train_list)}, Tests samples: {len(test_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "random_idx = np.random.randint(1, 25000, size=10)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "i = 1\n",
    "for idx in random_idx:\n",
    "    ax = fig.add_subplot(2, 5, i)\n",
    "    img = Image.open(train_list[idx])\n",
    "    plt.imshow(img)\n",
    "    i += 1\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Instances dog: {sum('dog.' in s for s in train_list)}, Instances Cat: {sum('cat.' in s for s in train_list)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogVsCatDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.file_list[idx]\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label_category = label = img_name.split(\"/\")[-1].split(\".\")[0]\n",
    "        if label_category == \"dog\":\n",
    "            label = 1\n",
    "        elif label_category == \"cat\":\n",
    "            label = 0\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore more transformations\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DogVsCatDataset(train_list, transform=train_transforms)\n",
    "test_data = DogVsCatDataset(test_list, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(3 * 3 * 64, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # Helps\n",
    "    # https://datascience.stackexchange.com/questions/40906/determining-size-of-fc-layer-after-conv-layer-in-pytorch\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIPS FOR FINE TUNNING\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(params=model.fc.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # get data\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # predictions\n",
    "        output = model(data)\n",
    "\n",
    "        # comput loss and backward\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # adjust weights\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == target).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1}, train accuracy : {epoch_accuracy}, train loss : {epoch_loss}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Epoch : {epoch+1}, train accuracy : {epoch_accuracy}, train loss : {epoch_loss}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, fileid in test_loader:\n",
    "        data = data.to(device)\n",
    "        preds = model(data)\n",
    "        preds_list = F.softmax(preds, dim=1)[:, 1].tolist()\n",
    "        probs += list(zip(list(fileid), preds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "random_idx = np.random.randint(1, len(test_loader), size=10)\n",
    "\n",
    "class_ = {0: \"cat\", 1: \"dog\"}\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "i = 1\n",
    "\n",
    "for idx in random_idx:\n",
    "    ax = fig.add_subplot(2, 5, i)\n",
    "\n",
    "    img_id = probs[idx][0]\n",
    "    pred = probs[idx][1]\n",
    "\n",
    "    if pred > 0.5:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "\n",
    "    img_path = os.path.join(test_dir, f\"{img_id}.jpg\")\n",
    "    img = Image.open(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(class_[label])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to do?\n",
    "\n",
    "- Change the code in order to improve the performance. Tips: add more transformation, increase the number of epochs, you can even try to change the model. Maybe you can also use a validation test, by spliting the train_list, and check the validation performance on the end of each epoch\n",
    "- Try to fine tune a model, the tips are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIPS FOR FINE TUNNING\n",
    "#  model_conv = models.resnet18(pretrained=True)\n",
    "# for param in model_conv.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Parameters of newly constructed modules have requires_grad=True by default\n",
    "# num_ftrs = model_conv.fc.in_features\n",
    "# model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# model_conv = model_conv.to(device)\n",
    "\n",
    "\n",
    "# model_conv.fc.parameters() #ADD THIS IN THE OPTIMIZER"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a7676564d16bc58d94363a5302f065cad7216a181e20771947af71c951c7aa1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
