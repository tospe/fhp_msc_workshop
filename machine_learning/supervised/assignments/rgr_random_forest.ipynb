{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import ensemble, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Intuition\n",
    "\n",
    "We have three datasets:\n",
    " - Diabetes dataset contains data from diabetic patients and contains certain features such as their bmi, age , blood pressure and glucose levels which are useful in predicting the diabetes disease progression in patients.\n",
    "- \"Perfect regression\" is the simpler case of regression\n",
    "- Noisy sin wave has a shape of sine wave with some noise\n",
    "\n",
    "This interactive demo lets you explore the Linear Regression algorithm. \n",
    "\n",
    "We can visualize the how the regressor fits the diferent datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest_regressor(n_estimators, criterion, max_depth, min_samples_split, X, y):\n",
    "    regr = ensemble.RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    regr.fit(X, y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that n_estimators defines the number of trees in the forest, `max_depth` refers to the maximum depth of the tree, `criterion` is the function to measure the quality of a split, and `min_sample_split` is the minimum number of samples required to split an internal node.\n",
    "\n",
    "Using the interactive demo try to answer the following questions:\n",
    "* What is the impact of the value of `n_estimators`, `max_depth` and `min_sample_split` in the prediction? How do they relate to the bias-variance tradeoff?\n",
    "\n",
    "* Do the different criterion have great impact on model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Dataset\n",
    "dataset = datasets.load_diabetes() # try with another dataset \n",
    "\n",
    "X = dataset.data[:, :2]\n",
    "y = dataset.target\n",
    "\n",
    "# Perfect regression\n",
    "X, y = datasets.make_regression(n_samples=100, n_features=1, random_state=0, noise=10.0, bias=100.0)\n",
    "\n",
    "# Sin wave \n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() \n",
    "y[::5] += 3 * (0.5 - rng.rand(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.canvas.header_visible = False\n",
    "plt.scatter(X[:, 0], y, color=\"darkorange\",s=30, alpha=0.6)\n",
    "x = np.linspace(min(X), max(X), len(X))\n",
    "\n",
    "\n",
    "lines = plt.plot(x, y, label=\"Random Forest Regression\")\n",
    "\n",
    "def plot_boundary(n_estimators, criterion, max_depth, min_samples_split):\n",
    "    regr = fit_random_forest_regressor(n_estimators, criterion, max_depth, min_samples_split, X, y)\n",
    "\n",
    "    y_preds = regr.predict(x)\n",
    "\n",
    "    lines[0].set_data(x, y_preds)\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "\n",
    "    plt.legend()\n",
    "    display(fig)\n",
    "    return regr\n",
    "\n",
    "\n",
    "inter = interactive(\n",
    "    plot_boundary,\n",
    "    n_estimators= (70, 140, 10),\n",
    "    criterion=[\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "    max_depth=(1,10,2),\n",
    "    min_samples_split=(2, 10, 2),\n",
    ")\n",
    "\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split the dataset in training and testing. Use a test_size of 33%\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instantiate a Random Forest Regressor\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest#sklearn.ensemble.RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the regressor to predict the test set.\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest#sklearn.ensemble.RandomForestRegressor.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the regressor error on the test set (also known as hold-out evaluation)\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html?highlight=rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra - Compare the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, prediction, ax=None):\n",
    "    min_value = min(np.min(y_test), np.min(prediction))*0.9\n",
    "    max_value = max(np.max(y_test), np.max(prediction))*1.1\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.plot([min_value,max_value], [min_value,max_value], color='grey')\n",
    "    ax.scatter(prediction, y_test, facecolor='steelblue', s=30, alpha=0.6)\n",
    "    ax.set_xlabel('Predicted', fontsize=12)\n",
    "    ax.set_ylabel('Actual', fontsize=12)\n",
    "    ax.set_xlim([min_value,max_value])\n",
    "    ax.set_ylim([min_value,max_value])\n",
    "    plt.show()\n",
    "\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a4b451a457872003cbdeeff551b7b0f1698ff7893c58bc0328979c08908391b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
