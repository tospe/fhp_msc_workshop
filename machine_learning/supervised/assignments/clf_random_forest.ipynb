{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import ensemble, datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Intuition\n",
    "\n",
    "The data set consists of 50 samples from each of three species of *Iris* ([Iris setosa](https://en.wikipedia.org/wiki/Iris_setosa), [Iris virginica](https://en.wikipedia.org/wiki/Iris_virginica) and [Iris versicolor](https://en.wikipedia.org/wiki/Iris_versicolor)). Four features were measured from each sample: the length and the width of the [sepals](https://en.wikipedia.org/wiki/Setal) and [petals](https://en.wikipedia.org/wiki/Petal), in centimeters.\n",
    "\n",
    "This interactive demo lets you explore the Random Forest model for classification. The demo uses a Random Forest model in the Iris dataset. \n",
    "\n",
    "We can visualize the classifier decision boundary. A decision boundary is a line (in the case of two features), where all (or most) samples of one class are on one side of that line, and all samples of the other class are on the opposite side of the line. The line separates one class from the other. If you have more than two features, the decision boundary is not a line, but a (hyper)-plane in the dimension of your feature space.\n",
    "Each point in the plane is colored with the class that would be assigned to it using the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree(n_estimators, max_depth, criterion, min_samples_split, X, y):\n",
    "    clf = ensemble.RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion=criterion, min_samples_split=min_samples_split)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris() # try with another dataset \n",
    "\n",
    "X = dataset.data[:, :2]\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69da18fcf6d4c3790a4a2de3cede70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='n_estimators', max=140, min=70, step=10), IntSlider(vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Create color maps\n",
    "plt.ioff()\n",
    "\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=dataset.target_names[y],\n",
    "    palette=cmap_bold,\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "def plot_boundary(n_estimators, max_depth, criterion, min_samples_split):\n",
    "    clf = fit_tree(n_estimators, max_depth, criterion, min_samples_split, X, y)\n",
    "    \n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        cmap=cmap_light,\n",
    "        ax=ax,\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        xlabel=dataset.feature_names[0],\n",
    "        ylabel=dataset.feature_names[1],\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        hue=dataset.target_names[y],\n",
    "        palette=cmap_bold,\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"black\",\n",
    "        legend=False\n",
    "    )\n",
    "    display(fig)\n",
    "        # Plot also the training point\n",
    "    return clf\n",
    "\n",
    "\n",
    "inter = interactive(\n",
    "    plot_boundary,\n",
    "    n_estimators= (70, 140, 10),\n",
    "    criterion=[\"gini\", \"entropy\", \"log_loss\"],\n",
    "    max_depth=(1,30, 7),\n",
    "    min_samples_split=(2, 20),\n",
    ")\n",
    "\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is draw using the sepal width and sepal length features. Recall that n_estimators defines the number of trees in the forest, `max_depth` refers to the maximum depth of the tree, `criterion` is the function to measure the quality of a split, and `min_sample_split` is the minimum number of samples required to split an internal node.\n",
    "\n",
    "Using the interactive demo try to answer the following questions:\n",
    "* What is the impact of the value of `n_estimators`, `max_depth` and `min_sample_split` in the prediction? How do they relate to the bias-variance tradeoff?\n",
    "\n",
    "* Do the different criterion have great impact on model performance?\n",
    "\n",
    "* Why are the decision boundaries straight lines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Train and Test\n",
    "\n",
    "In the next section you will use the Random Forest classifier to make predictions on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ensemble.RandomForestClassifier()\n",
    "clf.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongs = np.where((y_pred == y_test) == False)[0]\n",
    "print(f\"There are {len(wrongs)} wrong predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra - Inspect the misclassification(s)\n",
    "\n",
    "Run the code below and inspect the model's misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=dataset.target_names[y],\n",
    "    palette=cmap_bold,\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "plt.scatter(    \n",
    "    x=X_test[wrongs, 0],\n",
    "    y=X_test[wrongs, 1],\n",
    "    color=[cmap_bold[i] for i in y_pred[wrongs]],\n",
    "    marker=\"+\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a4b451a457872003cbdeeff551b7b0f1698ff7893c58bc0328979c08908391b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
