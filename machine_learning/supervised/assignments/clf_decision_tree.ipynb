{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import tree, datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Intuition\n",
    "\n",
    "The data set consists of 50 samples from each of three species of *Iris* ([Iris setosa](https://en.wikipedia.org/wiki/Iris_setosa), [Iris virginica](https://en.wikipedia.org/wiki/Iris_virginica) and [Iris versicolor](https://en.wikipedia.org/wiki/Iris_versicolor)). Four features were measured from each sample: the length and the width of the [sepals](https://en.wikipedia.org/wiki/Setal) and [petals](https://en.wikipedia.org/wiki/Petal), in centimeters.\n",
    "\n",
    "This interactive demo lets you explore the Decision Tree algorithm for classification. The demo uses a Decision Tree model in the Iris dataset. \n",
    "\n",
    "We can visualize the classifier decision boundary. A decision boundary is a line (in the case of two features), where all (or most) samples of one class are on one side of that line, and all samples of the other class are on the opposite side of the line. The line separates one class from the other. If you have more than two features, the decision boundary is not a line, but a (hyper)-plane in the dimension of your feature space.\n",
    "Each point in the plane is colored with the class that would be assigned to it using the Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree(max_depth, criterion, min_samples_split, X, y):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=max_depth, criterion=criterion, min_samples_split=min_samples_split)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris() # try with another dataset \n",
    "\n",
    "X = dataset.data[:, :2]\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.canvas.header_visible = True\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=dataset.target_names[y],\n",
    "    palette=cmap_bold,\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "def plot_boundary(max_depth, criterion, min_samples_split):\n",
    "    clf = fit_tree(max_depth, criterion, min_samples_split, X, y)\n",
    "    \n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        cmap=cmap_light,\n",
    "        ax=ax,\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        xlabel=dataset.feature_names[0],\n",
    "        ylabel=dataset.feature_names[1],\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        hue=dataset.target_names[y],\n",
    "        palette=cmap_bold,\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"black\",\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    display(fig)\n",
    "    return clf\n",
    "\n",
    "\n",
    "inter = interactive(\n",
    "    plot_boundary,\n",
    "    criterion=[\"gini\", \"entropy\", \"log_loss\"],\n",
    "    max_depth=(1,30, 7),\n",
    "    min_samples_split=(2, 20),\n",
    ")\n",
    "\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is draw using the sepal width and sepal length features. Recall that `max_depth` refers to the maximum depth of the tree, `criterion` is the function to measure the quality of a split, and `min_sample_split` is the minimum number of samples required to split an internal node.\n",
    "\n",
    "Using the interactive demo try to answer the following questions:\n",
    "* What is the impact of the value of `max_depth` and `min_sample_split` in the prediction? How do they relate to the bias-variance tradeoff?\n",
    "\n",
    "* Do the different criterion have great impact on model performance?\n",
    "\n",
    "* Why are the decision boundaries straight lines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Train and Test\n",
    "In the next section you will use the Decision Tree classifier to make predictions on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split the dataset in training and testing. Use a test_size of 33%\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instantiate a Decision Tree classifier with default parameters\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the classifier to predict the test set.\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the classifier error on the test set (also known as hold-out evaluation)\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra - Inspect the misclassification(s)\n",
    "\n",
    "Run the code below and inspect the model's misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=dataset.target_names[y],\n",
    "    palette=cmap_bold,\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "plt.scatter(    \n",
    "    x=X_test[wrongs, 0],\n",
    "    y=X_test[wrongs, 1],\n",
    "    color=[cmap_bold[i] for i in y_pred[wrongs]],\n",
    "    marker=\"+\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra - Visualize the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.canvas.resizable = True\n",
    "\n",
    "def plot_tree(crit, split, depth, min_split, min_leaf=1):\n",
    "    estimator = tree.DecisionTreeClassifier(\n",
    "        random_state=1,\n",
    "        criterion=crit,\n",
    "        splitter=split,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_split,\n",
    "        min_samples_leaf=min_leaf,\n",
    "    )\n",
    "\n",
    "    estimator.fit(X_train, y_train)\n",
    "\n",
    "    tree.plot_tree(\n",
    "        estimator,\n",
    "        feature_names=dataset[\"feature_names\"],\n",
    "        class_names=dataset[\"target_names\"],\n",
    "        filled=True,\n",
    "        fontsize=10,\n",
    "    )\n",
    "    display(fig)\n",
    "    return estimator\n",
    "\n",
    "\n",
    "inter = interactive(\n",
    "    plot_tree,\n",
    "    crit=[\"gini\", \"entropy\"],\n",
    "    split=[\"best\", \"random\"],\n",
    "    depth=[1, 2, 3, 4, 5, 10],\n",
    "    min_split=(2, 20),\n",
    "    min_leaf=(1, 10),\n",
    ")\n",
    "\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a4b451a457872003cbdeeff551b7b0f1698ff7893c58bc0328979c08908391b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
