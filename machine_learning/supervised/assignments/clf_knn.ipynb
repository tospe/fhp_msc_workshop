{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Intuition\n",
    "\n",
    "The data set consists of 50 samples from each of three species of *Iris* ([Iris setosa](https://en.wikipedia.org/wiki/Iris_setosa), [Iris virginica](https://en.wikipedia.org/wiki/Iris_virginica) and [Iris versicolor](https://en.wikipedia.org/wiki/Iris_versicolor)). Four features were measured from each sample: the length and the width of the [sepals](https://en.wikipedia.org/wiki/Setal) and [petals](https://en.wikipedia.org/wiki/Petal), in centimeters.\n",
    "\n",
    "This interactive demo lets you explore the K-Nearest Neighbors algorithm for classification. The demo uses a kNN model in the Iris dataset. \n",
    "\n",
    "We can visualize the classifier decision boundary. A decision boundary is a line (in the case of two features), where all (or most) samples of one class are on one side of that line, and all samples of the other class are on the opposite side of the line. The line separates one class from the other. If you have more than two features, the decision boundary is not a line, but a (hyper)-plane in the dimension of your feature space.\n",
    "Each point in the plane is colored with the class that would be assigned to it using the K-Nearest Neighbors algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_knn(n_neighbors, weights, X, y):\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris() # try with another dataset \n",
    "\n",
    "X = dataset.data[:, :2]\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create color maps\n",
    "plt.ioff()\n",
    "\n",
    "cmap_light = ListedColormap([\"orange\", \"cyan\", \"cornflowerblue\"])\n",
    "cmap_bold = [\"darkorange\", \"c\", \"darkblue\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.canvas.header_visible = False\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=dataset.target_names[y],\n",
    "    palette=cmap_bold,\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "def plot_boundary(n_neighbors, weights):\n",
    "    clf = fit_knn(n_neighbors, weights, X, y)\n",
    "    \n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        cmap=cmap_light,\n",
    "        ax=ax,\n",
    "        response_method=\"predict\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        xlabel=dataset.feature_names[0],\n",
    "        ylabel=dataset.feature_names[1],\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=X[:, 0],\n",
    "        y=X[:, 1],\n",
    "        hue=dataset.target_names[y],\n",
    "        palette=cmap_bold,\n",
    "        alpha=1.0,\n",
    "        edgecolor=\"black\",\n",
    "        legend=False\n",
    "    )\n",
    "    display(fig)\n",
    "    return clf\n",
    "\n",
    "\n",
    "inter = interactive(\n",
    "    plot_boundary,\n",
    "    weights=[\"uniform\", \"distance\"],\n",
    "    n_neighbors=(1, 10, 2),\n",
    ")\n",
    "\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is draw using the sepal width and sepal length features. Recall that `n_neighbors` refers to the number of neighbors to use by default for kneighbors queries and `weights` is used to implement a weight function used in prediction.\n",
    "\n",
    "Using the interactive demo try to answer the following questions:\n",
    "* What is the impact of the value of `n_neighbors` in the prediction? How does that relates to the bias-variance tradeoff?\n",
    "* In what extent the predictions with `uniform` and `distance` weighting differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Train and Test\n",
    "\n",
    "In the next section you will use the kNN classifier to make predictions on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split the dataset in training and testing. Use a test_size of 33%\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instantiate a kNN classifier with k=5 and uniform weighting \n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use the classifier to predict the test set.\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the classifier error on the test set (also known as hold-out evaluation)\n",
    "# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra - Inspect the misclassification(s)\n",
    "\n",
    "Run the code below and inspect the model's misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    hue=dataset.target_names[y],\n",
    "    palette=cmap_bold,\n",
    "    alpha=1.0,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "plt.scatter(    \n",
    "    x=X_test[wrongs, 0],\n",
    "    y=X_test[wrongs, 1],\n",
    "    color=[cmap_bold[i] for i in y_pred[wrongs]],\n",
    "    marker=\"+\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a4b451a457872003cbdeeff551b7b0f1698ff7893c58bc0328979c08908391b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
